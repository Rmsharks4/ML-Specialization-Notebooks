{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise-1: XGBoost: Fit/Predict\n",
    "It's time to create your first XGBoost model! As Sergey showed you in the video, you can use the scikit-learn .fit() / .predict() paradigm that you are already familiar to build your XGBoost models, as the xgboost library has a scikit-learn compatible API!\n",
    "\n",
    "Here, you'll be working with churn data. This dataset contains imaginary data from a ride-sharing app with user behaviors over their first month of app usage in a set of imaginary cities as well as whether they used the service 5 months after sign-up. It has been pre-loaded for you into a DataFrame called churn_data - explore it in the Shell!\n",
    "\n",
    "Your goal is to use the first month's worth of data to predict whether the app's users will remain users of the service at the 5 month mark. This is a typical setup for a churn prediction problem. To do this, you'll split the data into training and test sets, fit a small xgboost model on the training set, and evaluate its performance on the test set by computing its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_dist</th>\n",
       "      <th>avg_rating_by_driver</th>\n",
       "      <th>avg_rating_of_driver</th>\n",
       "      <th>avg_inc_price</th>\n",
       "      <th>inc_pct</th>\n",
       "      <th>weekday_pct</th>\n",
       "      <th>fancy_car_user</th>\n",
       "      <th>city_Carthag</th>\n",
       "      <th>city_Harko</th>\n",
       "      <th>phone_iPhone</th>\n",
       "      <th>first_month_cat_more_1_trip</th>\n",
       "      <th>first_month_cat_no_trips</th>\n",
       "      <th>month_5_still_here</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.67</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.10</td>\n",
       "      <td>15.4</td>\n",
       "      <td>46.2</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.26</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.77</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.36</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.14</td>\n",
       "      <td>20.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.13</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.19</td>\n",
       "      <td>11.8</td>\n",
       "      <td>82.4</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   avg_dist  avg_rating_by_driver  avg_rating_of_driver  avg_inc_price  \\\n",
       "0                                                                        \n",
       "0      3.67                   5.0                   4.7           1.10   \n",
       "1      8.26                   5.0                   5.0           1.00   \n",
       "2      0.77                   5.0                   4.3           1.00   \n",
       "3      2.36                   4.9                   4.6           1.14   \n",
       "4      3.13                   4.9                   4.4           1.19   \n",
       "\n",
       "   inc_pct  weekday_pct  fancy_car_user  city_Carthag  city_Harko  \\\n",
       "0                                                                   \n",
       "0     15.4         46.2            True             0           1   \n",
       "1      0.0         50.0           False             1           0   \n",
       "2      0.0        100.0           False             1           0   \n",
       "3     20.0         80.0            True             0           1   \n",
       "4     11.8         82.4           False             0           0   \n",
       "\n",
       "   phone_iPhone  first_month_cat_more_1_trip  first_month_cat_no_trips  \\\n",
       "0                                                                        \n",
       "0             1                            1                         0   \n",
       "1             0                            0                         1   \n",
       "2             1                            1                         0   \n",
       "3             1                            1                         0   \n",
       "4             0                            1                         0   \n",
       "\n",
       "   month_5_still_here  \n",
       "0                      \n",
       "0                   1  \n",
       "1                   0  \n",
       "2                   0  \n",
       "3                   1  \n",
       "4                   0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "churn_data = pd.read_csv('churn_data.csv', index_col=0)\n",
    "churn_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.1.0-py3-none-win_amd64.whl (37.7 MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\rmsha\\anaconda3\\lib\\site-packages (from xgboost) (1.16.6)\n",
      "Requirement already satisfied: scipy in c:\\users\\rmsha\\anaconda3\\lib\\site-packages (from xgboost) (1.4.1)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.758200\n"
     ]
    }
   ],
   "source": [
    "# Import xgboost\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Create arrays for the features and the target: X, y\n",
    "X, y = churn_data.iloc[:,:-1], churn_data.iloc[:,-1]\n",
    "\n",
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Instantiate the XGBClassifier: xg_cl\n",
    "xg_cl = xgb.XGBClassifier(objective='binary:logistic', n_estimators=10, seed=123)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "xg_cl.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "preds = xg_cl.predict(X_test)\n",
    "\n",
    "# Compute the accuracy: accuracy\n",
    "accuracy = float(np.sum(preds==y_test))/y_test.shape[0]\n",
    "print(\"accuracy: %f\" % (accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise-2: Decision trees\n",
    "Your task in this exercise is to make a simple decision tree using scikit-learn's DecisionTreeClassifier on the breast cancer dataset that comes pre-loaded with scikit-learn.\n",
    "\n",
    "This dataset contains numeric measurements of various dimensions of individual tumors (such as perimeter and texture) from breast biopsies and a single outcome value (the tumor is either malignant, or benign).\n",
    "\n",
    "We've preloaded the dataset of samples (measurements) into X and the target values per tumor into y. Now, you have to split the complete dataset into training and testing sets, and then train a DecisionTreeClassifier. You'll specify a parameter called max_depth. Many other parameters can be modified within this model, and you can check all of them out here (https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "X, y = breast_cancer.data, breast_cancer.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Instantiate the classifier: dt_clf_4\n",
    "dt_clf_4 = DecisionTreeClassifier(max_depth=4)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "dt_clf_4.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred_4\n",
    "y_pred_4 = dt_clf_4.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the predictions: accuracy\n",
    "accuracy = float(np.sum(y_pred_4==y_test))/y_test.shape[0]\n",
    "print(\"accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise-3: Measuring accuracy\n",
    "You'll now practice using XGBoost's learning API through its baked in cross-validation capabilities. As Sergey discussed in the previous video, XGBoost gets its lauded performance and efficiency gains by utilizing its own optimized data structure for datasets called a DMatrix.\n",
    "\n",
    "In the previous exercise, the input datasets were converted into DMatrix data on the fly, but when you use the xgboost cv object, you have to first explicitly convert your data into a DMatrix. So, that's what you will do here before running cross-validation on churn_data.\n",
    "\n",
    "Perform 3-fold cross-validation by calling xgb.cv(). dtrain is your churn_dmatrix, params is your parameter dictionary, nfold is the number of cross-validation folds (3), num_boost_round is the number of trees we want to build (5), metrics is the metric you want to compute (this will be \"error\", which we will convert to an accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train-error-mean  train-error-std  test-error-mean  test-error-std\n",
      "0           0.28232         0.002366          0.28378        0.001932\n",
      "1           0.26951         0.001855          0.27190        0.001932\n",
      "2           0.25605         0.003213          0.25798        0.003963\n",
      "3           0.25090         0.001845          0.25434        0.003827\n",
      "4           0.24654         0.001981          0.24852        0.000934\n",
      "0.75148\n"
     ]
    }
   ],
   "source": [
    "# Create arrays for the features and the target: X, y\n",
    "X, y = churn_data.iloc[:,:-1], churn_data.iloc[:,-1]\n",
    "\n",
    "# Create the DMatrix from X and y: churn_dmatrix\n",
    "churn_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:logistic\", \"max_depth\":3}\n",
    "\n",
    "# Perform cross-validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, \n",
    "                  nfold=3, num_boost_round=5, \n",
    "                  metrics=\"error\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Print the accuracy\n",
    "print(((1-cv_results[\"test-error-mean\"]).iloc[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise-4: Measuring AUC\n",
    "Now that you've used cross-validation to compute average out-of-sample accuracy (after converting from an error), it's very easy to compute any other metric you might be interested in. All you have to do is pass it (or a list of metrics) in as an argument to the metrics parameter of xgb.cv().\n",
    "\n",
    "Your job in this exercise is to compute another common metric used in binary classification - the area under the curve (\"auc\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train-auc-mean  train-auc-std  test-auc-mean  test-auc-std\n",
      "0        0.768893       0.001544       0.767863      0.002820\n",
      "1        0.790864       0.006758       0.789157      0.006846\n",
      "2        0.815872       0.003900       0.814476      0.005997\n",
      "3        0.822959       0.002018       0.821682      0.003912\n",
      "4        0.827528       0.000769       0.826191      0.001937\n",
      "0.826191\n"
     ]
    }
   ],
   "source": [
    "# Perform cross_validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, \n",
    "                  nfold=3, num_boost_round=5, \n",
    "                  metrics=\"auc\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Print the AUC\n",
    "print((cv_results[\"test-auc-mean\"]).iloc[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise-5: Decision trees as base learners\n",
    "It's now time to build an XGBoost model to predict house prices - not in Boston, Massachusetts, as you saw in the video, but in Ames, Iowa! This dataset of housing prices has been pre-loaded into a DataFrame called df. \n",
    "\n",
    "In this exercise, your goal is to use trees as base learners. By default, XGBoost uses trees as base learners, so you don't have to specify that you want to use trees here with booster=\"gbtree\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = datasets.load_boston()\n",
    "X, y = df.data, df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 3.782443\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Instantiate the XGBRegressor: xg_reg\n",
    "xg_reg = xgb.XGBRegressor(seed=123, objective='reg:squarederror', n_estimators=10)\n",
    "\n",
    "# Fit the regressor to the training set\n",
    "xg_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "preds = xg_reg.predict(X_test)\n",
    "\n",
    "# Compute the rmse: rmse\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise-6: Linear base learners\n",
    "Now that you've used trees as base models in XGBoost, let's use the other kind of base model that can be used with XGBoost - a linear learner. This model, although not as commonly used in XGBoost, allows you to create a regularized linear regression using XGBoost's powerful learning API. However, because it's uncommon, you have to use XGBoost's own non-scikit-learn compatible functions to build the model, such as xgb.train().\n",
    "\n",
    "In order to do this you must create the parameter dictionary that describes the kind of booster you want to use (similarly to how you created the dictionary in Chapter 1 when you used xgb.cv()). The key-value pair that defines the booster type (base model) you need is \"booster\":\"gblinear\".\n",
    "\n",
    "Once you've created the model, you can use the .train() and .predict() methods of the model just like you've done in the past.\n",
    "\n",
    "Here, the data has already been split into training and testing sets, so you can dive right into creating the DMatrix objects required by the XGBoost learning API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>Remodeled</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <th>BsmtHalfBath</th>\n",
       "      <th>...</th>\n",
       "      <th>HouseStyle_1.5Unf</th>\n",
       "      <th>HouseStyle_1Story</th>\n",
       "      <th>HouseStyle_2.5Fin</th>\n",
       "      <th>HouseStyle_2.5Unf</th>\n",
       "      <th>HouseStyle_2Story</th>\n",
       "      <th>HouseStyle_SFoyer</th>\n",
       "      <th>HouseStyle_SLvl</th>\n",
       "      <th>PavedDrive_P</th>\n",
       "      <th>PavedDrive_Y</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>1710</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>0</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>1786</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1</td>\n",
       "      <td>1717</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>2198</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0          60         65.0     8450            7            5       2003   \n",
       "1          20         80.0     9600            6            8       1976   \n",
       "2          60         68.0    11250            7            5       2001   \n",
       "3          70         60.0     9550            7            5       1915   \n",
       "4          60         84.0    14260            8            5       2000   \n",
       "\n",
       "   Remodeled  GrLivArea  BsmtFullBath  BsmtHalfBath  ...  HouseStyle_1.5Unf  \\\n",
       "0          0       1710             1             0  ...                  0   \n",
       "1          0       1262             0             1  ...                  0   \n",
       "2          1       1786             1             0  ...                  0   \n",
       "3          1       1717             1             0  ...                  0   \n",
       "4          0       2198             1             0  ...                  0   \n",
       "\n",
       "   HouseStyle_1Story  HouseStyle_2.5Fin  HouseStyle_2.5Unf  HouseStyle_2Story  \\\n",
       "0                  0                  0                  0                  1   \n",
       "1                  1                  0                  0                  0   \n",
       "2                  0                  0                  0                  1   \n",
       "3                  0                  0                  0                  1   \n",
       "4                  0                  0                  0                  1   \n",
       "\n",
       "   HouseStyle_SFoyer  HouseStyle_SLvl  PavedDrive_P  PavedDrive_Y  SalePrice  \n",
       "0                  0                0             0             1     208500  \n",
       "1                  0                0             0             1     181500  \n",
       "2                  0                0             0             1     223500  \n",
       "3                  0                0             0             1     140000  \n",
       "4                  0                0             0             1     250000  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ames = pd.read_csv('ames_housing_trimmed_processed.csv')\n",
    "ames.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(ames.drop('SalePrice', axis=1), ames['SalePrice'], \n",
    "                                                    test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 42209.970974\n"
     ]
    }
   ],
   "source": [
    "# Convert the training and testing sets into DMatrixes: DM_train, DM_test\n",
    "DM_train = xgb.DMatrix(X_train, y_train)\n",
    "DM_test =  xgb.DMatrix(X_test, y_test)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"booster\":\"gblinear\", \"objective\":\"reg:squarederror\"}\n",
    "\n",
    "# Train the model: xg_reg\n",
    "xg_reg = xgb.train(params = params, dtrain=DM_train, num_boost_round=5)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "preds = xg_reg.predict(DM_test)\n",
    "\n",
    "# Compute and print the RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_test,preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise-7: Evaluating model quality\n",
    "It's now time to begin evaluating model quality.\n",
    "\n",
    "Here, you will compare the RMSE and MAE of a cross-validated XGBoost model on the Ames housing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
      "0    141767.535156      429.451316   142980.433594    1193.791602\n",
      "1    102832.542969      322.473304   104891.392578    1223.157623\n",
      "2     75872.617188      266.469946    79478.939454    1601.341377\n",
      "3     57245.650390      273.623926    62411.921875    2220.149857\n",
      "4     44401.298828      316.423666    51348.281250    2963.378741\n",
      "4    51348.28125\n",
      "Name: test-rmse-mean, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=ames.drop('SalePrice', axis=1), label=ames['SalePrice'])\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:squarederror\", \"max_depth\":4}\n",
    "\n",
    "# Perform cross-validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, metrics='rmse', \n",
    "                    as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Extract and print final boosting round metric\n",
    "print((cv_results[\"test-rmse-mean\"]).tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train-mae-mean  train-mae-std  test-mae-mean  test-mae-std\n",
      "0   127343.480469     668.337982  127633.972657   2404.002681\n",
      "1    89770.052735     456.957620   90122.501953   2107.912682\n",
      "2    63580.790039     263.405712   64278.563477   1887.565119\n",
      "3    45633.153321     151.884551   46819.166016   1459.819398\n",
      "4    33587.093750      86.999137   35670.645508   1140.606558\n",
      "4    35670.645508\n",
      "Name: test-mae-mean, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=ames.drop('SalePrice', axis=1), label=ames['SalePrice'])\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:squarederror\", \"max_depth\":4}\n",
    "\n",
    "# Perform cross-validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, metrics='mae', \n",
    "                    as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Extract and print final boosting round metric\n",
    "print((cv_results[\"test-mae-mean\"]).tail(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise-8: Using regularization in XGBoost\n",
    "Having seen an example of l1 regularization in the video, you'll now vary the l2 regularization penalty - also known as \"lambda\" - and see its effect on overall model performance on the Ames housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best rmse as a function of l2:\n",
      "    l2          rmse\n",
      "0    1  52275.357422\n",
      "1   10  57746.062500\n",
      "2  100  76624.625001\n"
     ]
    }
   ],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=ames.drop('SalePrice', axis=1), label=ames['SalePrice'])\n",
    "\n",
    "reg_params = [1, 10, 100]\n",
    "\n",
    "# Create the initial parameter dictionary for varying l2 strength: params\n",
    "params = {\"objective\":\"reg:squarederror\",\"max_depth\":3}\n",
    "\n",
    "# Create an empty list for storing rmses as a function of l2 complexity\n",
    "rmses_l2 = []\n",
    "\n",
    "# Iterate over reg_params\n",
    "for reg in reg_params:\n",
    "\n",
    "    # Update l2 strength\n",
    "    params[\"lambda\"] = reg\n",
    "    \n",
    "    # Pass this updated param dictionary into cv\n",
    "    cv_results_rmse = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2, num_boost_round=5, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append best rmse (final round) to rmses_l2\n",
    "    rmses_l2.append(cv_results_rmse[\"test-rmse-mean\"].tail(1).values[0])\n",
    "\n",
    "# Look at best rmse per l2 param\n",
    "print(\"Best rmse as a function of l2:\")\n",
    "print(pd.DataFrame(list(zip(reg_params, rmses_l2)), columns=[\"l2\", \"rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise-9: Visualizing individual XGBoost trees\n",
    "Now that you've used XGBoost to both build and evaluate regression as well as classification models, you should get a handle on how to visually explore your models. Here, you will visualize individual trees from the fully boosted model that XGBoost creates using the entire housing dataset.\n",
    "\n",
    "XGBoost has a plot_tree() function that makes this type of visualization easy. Once you train a model using the XGBoost learning API, you can pass it to the plot_tree() function along with the number of trees you want to plot using the num_trees argument.\n",
    "\n",
    "```python\n",
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=ames.drop('SalePrice', axis=1), label=ames['SalePrice'])\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:squarederror\", \"max_depth\":2}\n",
    "\n",
    "# Train the model: xg_reg\n",
    "xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)\n",
    "\n",
    "# Plot the first tree\n",
    "xgb.plot_tree(xg_reg, num_trees=0)\n",
    "plt.show()\n",
    "\n",
    "# Plot the fifth tree\n",
    "xgb.plot_tree(xg_reg, num_trees=4)\n",
    "plt.show()\n",
    "\n",
    "# Plot the last tree sideways\n",
    "xgb.plot_tree(xg_reg, num_trees=9, rankdir='LR')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "<img src='plot1.svg' />\n",
    "<img src='plot5.svg' />\n",
    "<img src='plot10.svg' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise-10: Visualizing feature importances: What features are most important in my dataset\n",
    "Another way to visualize your XGBoost models is to examine the importance of each feature column in the original dataset within the model.\n",
    "\n",
    "One simple way of doing this involves counting the number of times each feature is split on across all boosting rounds (trees) in the model, and then visualizing the result as a bar graph, with the features ordered according to how many times they appear. XGBoost has a plot_importance() function that allows you to do exactly this, and you'll get a chance to use it in this exercise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=ames.drop('SalePrice', axis=1), label=ames['SalePrice'])\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\n",
    "    'objective':'reg:squarederror',\n",
    "    'max_depth':4\n",
    "}\n",
    "\n",
    "# Train the model: xg_reg\n",
    "xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)\n",
    "\n",
    "# Plot the feature importances\n",
    "xgb.plot_importance(xg_reg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise-11: Tuning the number of boosting rounds\n",
    "Let's start with parameter tuning by seeing how the number of boosting rounds (number of trees you build) impacts the out-of-sample performance of your XGBoost model. You'll use xgb.cv() inside a for loop and build one model per num_boost_round parameter.\n",
    "\n",
    "Here, you'll continue working with the Ames housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   num_boosting_rounds          rmse\n",
      "0                    5  50903.298177\n",
      "1                   10  34774.192709\n",
      "2                   15  32895.097656\n"
     ]
    }
   ],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=ames.drop('SalePrice', axis=1), label=ames['SalePrice'])\n",
    "\n",
    "# Create the parameter dictionary for each tree: params \n",
    "params = {\"objective\":\"reg:squarederror\", \"max_depth\":3}\n",
    "\n",
    "# Create list of number of boosting rounds\n",
    "num_rounds = [5, 10, 15]\n",
    "\n",
    "# Empty list to store final round rmse per XGBoost model\n",
    "final_rmse_per_round = []\n",
    "\n",
    "# Iterate over num_rounds and build one model per num_boost_round parameter\n",
    "for curr_num_rounds in num_rounds:\n",
    "\n",
    "    # Perform cross-validation: cv_results\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, num_boost_round=curr_num_rounds, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append final round RMSE\n",
    "    final_rmse_per_round.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "num_rounds_rmses = list(zip(num_rounds, final_rmse_per_round))\n",
    "print(pd.DataFrame(num_rounds_rmses,columns=[\"num_boosting_rounds\",\"rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise-12: Automated boosting round selection using early_stopping\n",
    "Now, instead of attempting to cherry pick the best possible number of boosting rounds, you can very easily have XGBoost automatically select the number of boosting rounds for you within xgb.cv(). This is done using a technique called early stopping.\n",
    "\n",
    "Early stopping works by testing the XGBoost model after every boosting round against a hold-out dataset and stopping the creation of additional boosting rounds (thereby finishing training of the model early) if the hold-out metric (\"rmse\" in our case) does not improve for a given number of rounds. Here you will use the early_stopping_rounds parameter in xgb.cv() with a large possible number of boosting rounds (50). Bear in mind that if the holdout metric continuously improves up through when num_boost_rounds is reached, then early stopping does not occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
      "0     141871.635417      403.636200   142640.656250     705.559400\n",
      "1     103057.036459       73.769561   104907.666667     111.114933\n",
      "2      75975.968750      253.729627    79262.057292     563.763447\n",
      "3      57420.531250      521.656755    61620.135417    1087.693857\n",
      "4      44552.955729      544.170190    50437.559896    1846.446535\n",
      "5      35763.947917      681.797248    43035.661458    2034.469207\n",
      "6      29861.463542      769.572072    38600.881510    2169.798065\n",
      "7      25994.675781      756.521419    36071.817708    2109.795430\n",
      "8      23306.836588      759.238254    34383.184896    1934.546688\n",
      "9      21459.769531      745.624999    33509.141276    1887.375284\n",
      "10     20148.721354      749.612769    32916.807943    1850.894476\n",
      "11     19215.382813      641.387565    32197.833333    1734.456784\n",
      "12     18627.389323      716.256794    31770.852865    1802.154800\n",
      "13     17960.694661      557.043073    31482.782552    1779.123767\n",
      "14     17559.736979      631.412969    31389.990886    1892.319967\n",
      "15     17205.712891      590.171393    31302.882162    1955.165902\n",
      "16     16876.571940      703.631755    31234.058594    1880.705796\n",
      "17     16597.662110      703.677609    31318.348959    1828.860617\n",
      "18     16330.460938      607.274493    31323.633464    1775.908279\n",
      "19     16005.972982      520.470911    31204.134766    1739.075860\n",
      "20     15814.301107      518.604304    31089.863281    1756.022288\n",
      "21     15493.405924      505.615987    31047.996094    1624.673955\n",
      "22     15270.734049      502.019527    31056.916016    1668.042813\n",
      "23     15086.381836      503.912899    31024.984375    1548.985605\n",
      "24     14917.608724      486.206468    30983.684245    1663.129605\n",
      "25     14709.590169      449.668438    30989.477865    1686.667141\n",
      "26     14457.286133      376.787666    30952.113281    1613.173550\n",
      "27     14185.567383      383.102234    31066.902344    1648.534310\n",
      "28     13934.066732      473.465449    31095.641927    1709.224745\n",
      "29     13749.645182      473.670437    31103.886068    1778.879154\n",
      "30     13549.836263      454.898488    30976.085287    1744.514533\n",
      "31     13413.485351      399.603618    30938.469401    1746.053896\n",
      "32     13275.916341      415.409108    30931.000651    1772.469906\n",
      "33     13085.878255      493.792509    30929.055990    1765.541073\n",
      "34     12947.181315      517.789250    30890.630208    1786.511479\n",
      "35     12846.027018      547.732805    30884.492187    1769.728223\n",
      "36     12702.378907      505.522877    30833.542969    1691.002065\n",
      "37     12532.244141      508.298241    30856.688151    1771.446377\n",
      "38     12384.054362      536.225108    30818.016927    1782.784630\n",
      "39     12198.444336      545.165562    30839.393229    1847.326009\n",
      "40     12054.583333      508.841412    30776.966146    1912.780507\n",
      "41     11897.036133      477.177991    30794.701823    1919.674347\n",
      "42     11756.221680      502.992782    30780.955078    1906.818667\n",
      "43     11618.846354      519.837088    30783.755860    1951.260705\n",
      "44     11484.080078      578.428250    30776.731120    1953.446310\n",
      "45     11356.553060      565.368380    30758.543620    1947.454953\n",
      "46     11193.558594      552.298906    30729.971354    1985.700237\n",
      "47     11071.315429      604.089960    30732.663411    1966.997809\n",
      "48     10950.777995      574.863209    30712.240886    1957.751118\n",
      "49     10824.865885      576.665756    30720.854818    1950.511520\n"
     ]
    }
   ],
   "source": [
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=ames.drop('SalePrice', axis=1), label=ames['SalePrice'])\n",
    "\n",
    "# Create the parameter dictionary for each tree: params\n",
    "params = {\"objective\":\"reg:squarederror\", \"max_depth\":4}\n",
    "\n",
    "# Perform cross-validation with early stopping: cv_results\n",
    "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, metrics='rmse',  early_stopping_rounds=10, \n",
    "                    num_boost_round=50, as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise-13: Tuning eta\n",
    "It's time to practice tuning other XGBoost hyperparameters in earnest and observing their effect on model performance! You'll begin by tuning the \"eta\", also known as the learning rate.\n",
    "\n",
    "The learning rate in XGBoost is a parameter that can range between 0 and 1, with higher values of \"eta\" penalizing feature weights more strongly, causing much stronger regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     eta      best_rmse\n",
      "0  0.001  195736.406250\n",
      "1  0.010  179932.187500\n",
      "2  0.100   79759.411459\n"
     ]
    }
   ],
   "source": [
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=ames.drop('SalePrice', axis=1), label=ames['SalePrice'])\n",
    "\n",
    "# Create the parameter dictionary for each tree (boosting round)\n",
    "params = {\"objective\":\"reg:squarederror\", \"max_depth\":3}\n",
    "\n",
    "# Create list of eta values and empty list to store final round rmse per xgboost model\n",
    "eta_vals = [0.001, 0.01, 0.1]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the eta \n",
    "for curr_val in eta_vals:\n",
    "\n",
    "    params[\"eta\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation: cv_results\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, metrics='rmse', early_stopping_rounds=5, \n",
    "                        num_boost_round=10, as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(eta_vals, best_rmse)), columns=[\"eta\",\"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise-14: Tuning max_depth\n",
    "In this exercise, your job is to tune max_depth, which is the parameter that dictates the maximum depth that each tree in a boosting round can grow to. Smaller values will lead to shallower trees, and larger values to deeper trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   max_depth     best_rmse\n",
      "0          2  37957.468750\n",
      "1          5  35596.599610\n",
      "2         10  36065.548829\n",
      "3         20  36739.578125\n"
     ]
    }
   ],
   "source": [
    "# Create your housing DMatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=ames.drop('SalePrice', axis=1), label=ames['SalePrice'])\n",
    "\n",
    "# Create the parameter dictionary\n",
    "params = {\"objective\":\"reg:squarederror\"}\n",
    "\n",
    "# Create list of max_depth values\n",
    "max_depths = [2,5,10,20]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the max_depth\n",
    "for curr_val in max_depths:\n",
    "\n",
    "    params[\"max_depth\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2, metrics='rmse', early_stopping_rounds=5, num_boost_round=10, as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(max_depths, best_rmse)),columns=[\"max_depth\",\"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise-15: Tuning colsample_bytree\n",
    "Now, it's time to tune \"colsample_bytree\". You've already seen this if you've ever worked with scikit-learn's RandomForestClassifier or RandomForestRegressor, where it just was called max_features. In both xgboost and sklearn, this parameter (although named differently) simply specifies the fraction of features to choose from at every split in a given tree. In xgboost, colsample_bytree must be specified as a float between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   colsample_bytree     best_rmse\n",
      "0               0.1  40918.117188\n",
      "1               0.5  35813.906250\n",
      "2               0.8  35995.677735\n",
      "3               1.0  35836.042969\n"
     ]
    }
   ],
   "source": [
    "# Create your housing DMatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=ames.drop('SalePrice', axis=1), label=ames['SalePrice'])\n",
    "\n",
    "# Create the parameter dictionary\n",
    "params={\"objective\":\"reg:squarederror\",\"max_depth\":3}\n",
    "\n",
    "# Create list of hyperparameter values: colsample_bytree_vals\n",
    "colsample_bytree_vals = [0.1,0.5,0.8,1]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the hyperparameter value \n",
    "for curr_val in colsample_bytree_vals:\n",
    "\n",
    "    params['colsample_bytree'] = curr_val\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2,\n",
    "                 num_boost_round=10, early_stopping_rounds=5,\n",
    "                 metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(colsample_bytree_vals, best_rmse)), columns=[\"colsample_bytree\",\"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise-16: Grid search with XGBoost\n",
    "Now that you've learned how to tune parameters individually with XGBoost, let's take your parameter tuning to the next level by using scikit-learn's GridSearch and RandomizedSearch capabilities with internal cross-validation using the GridSearchCV and RandomizedSearchCV functions. You will use these to find the best model exhaustively from a collection of possible parameter values across multiple parameters simultaneously. Let's get to work, starting with GridSearchCV!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y =ames.drop('SalePrice', axis=1), ames['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 4 candidates, totalling 16 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'colsample_bytree': 0.3, 'max_depth': 5, 'n_estimators': 50}\n",
      "Lowest RMSE found:  28986.18703093561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  16 out of  16 | elapsed:    0.7s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the parameter grid: gbm_param_grid\n",
    "gbm_param_grid = {\n",
    "    'colsample_bytree': [0.3, 0.7],\n",
    "    'n_estimators': [50],\n",
    "    'max_depth': [2, 5]\n",
    "}\n",
    "\n",
    "# Instantiate the regressor: gbm\n",
    "gbm = xgb.XGBRegressor()\n",
    "\n",
    "# Perform grid search: grid_mse\n",
    "grid_mse = GridSearchCV(estimator=gbm, param_grid=gbm_param_grid, scoring='neg_mean_squared_error', cv=4, verbose=1)\n",
    "\n",
    "# Fit grid_mse to the data\n",
    "grid_mse.fit(X, y)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \", grid_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise-17: Random search with XGBoost\n",
    "Often, GridSearchCV can be really time consuming, so in practice, you may want to use RandomizedSearchCV instead, as you will do in this exercise. The good news is you only have to make a few modifications to your GridSearchCV code to do RandomizedSearchCV. The key difference is you have to specify a param_distributions parameter instead of a param_grid parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 5 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'n_estimators': 25, 'max_depth': 4}\n",
      "Lowest RMSE found:  29998.4522530019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    1.1s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Create the parameter grid: gbm_param_grid \n",
    "gbm_param_grid = {\n",
    "    'n_estimators': [25],\n",
    "    'max_depth': range(2, 12)\n",
    "}\n",
    "\n",
    "# Instantiate the regressor: gbm\n",
    "gbm = xgb.XGBRegressor(n_estimators=10)\n",
    "\n",
    "# Perform random search: grid_mse\n",
    "randomized_mse = RandomizedSearchCV(estimator=gbm, param_distributions=gbm_param_grid, scoring='neg_mean_squared_error', \n",
    "                                    n_iter=5, cv=4, verbose=1)\n",
    "\n",
    "# Fit randomized_mse to the data\n",
    "randomized_mse.fit(X, y)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \", randomized_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(randomized_mse.best_score_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise-18: Encoding categorical columns I: LabelEncoder\n",
    "Now that you've seen what will need to be done to get the housing data ready for XGBoost, let's go through the process step-by-step.\n",
    "\n",
    "First, you will need to fill in missing values - as you saw previously, the column LotFrontage has many missing values. Then, you will need to encode any categorical columns in the dataset using one-hot encoding so that they are encoded numerically. \n",
    "\n",
    "The data has five categorical columns: MSZoning, PavedDrive, Neighborhood, BldgType, and HouseStyle. Scikit-learn has a LabelEncoder (https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) function that converts the values in each categorical column into integers. You'll practice using this here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Neighborhood</th>\n",
       "      <th>BldgType</th>\n",
       "      <th>HouseStyle</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>...</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <th>BsmtHalfBath</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>HalfBath</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>Fireplaces</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>PavedDrive</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>CollgCr</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>...</td>\n",
       "      <td>1710</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>548</td>\n",
       "      <td>Y</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Veenker</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>1Story</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>...</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>460</td>\n",
       "      <td>Y</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>CollgCr</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>...</td>\n",
       "      <td>1786</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>608</td>\n",
       "      <td>Y</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Crawfor</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>...</td>\n",
       "      <td>1717</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>642</td>\n",
       "      <td>Y</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>NoRidge</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>...</td>\n",
       "      <td>2198</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>836</td>\n",
       "      <td>Y</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass MSZoning  LotFrontage  LotArea Neighborhood BldgType HouseStyle  \\\n",
       "0          60       RL         65.0     8450      CollgCr     1Fam     2Story   \n",
       "1          20       RL         80.0     9600      Veenker     1Fam     1Story   \n",
       "2          60       RL         68.0    11250      CollgCr     1Fam     2Story   \n",
       "3          70       RL         60.0     9550      Crawfor     1Fam     2Story   \n",
       "4          60       RL         84.0    14260      NoRidge     1Fam     2Story   \n",
       "\n",
       "   OverallQual  OverallCond  YearBuilt  ...  GrLivArea  BsmtFullBath  \\\n",
       "0            7            5       2003  ...       1710             1   \n",
       "1            6            8       1976  ...       1262             0   \n",
       "2            7            5       2001  ...       1786             1   \n",
       "3            7            5       1915  ...       1717             1   \n",
       "4            8            5       2000  ...       2198             1   \n",
       "\n",
       "   BsmtHalfBath  FullBath  HalfBath  BedroomAbvGr  Fireplaces  GarageArea  \\\n",
       "0             0         2         1             3           0         548   \n",
       "1             1         2         0             3           1         460   \n",
       "2             0         2         1             3           1         608   \n",
       "3             0         1         0             3           1         642   \n",
       "4             0         2         1             4           1         836   \n",
       "\n",
       "   PavedDrive SalePrice  \n",
       "0           Y    208500  \n",
       "1           Y    181500  \n",
       "2           Y    223500  \n",
       "3           Y    140000  \n",
       "4           Y    250000  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('ames_unprocessed_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1460 entries, 0 to 1459\n",
      "Data columns (total 21 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   MSSubClass    1460 non-null   int64  \n",
      " 1   MSZoning      1460 non-null   object \n",
      " 2   LotFrontage   1201 non-null   float64\n",
      " 3   LotArea       1460 non-null   int64  \n",
      " 4   Neighborhood  1460 non-null   object \n",
      " 5   BldgType      1460 non-null   object \n",
      " 6   HouseStyle    1460 non-null   object \n",
      " 7   OverallQual   1460 non-null   int64  \n",
      " 8   OverallCond   1460 non-null   int64  \n",
      " 9   YearBuilt     1460 non-null   int64  \n",
      " 10  Remodeled     1460 non-null   int64  \n",
      " 11  GrLivArea     1460 non-null   int64  \n",
      " 12  BsmtFullBath  1460 non-null   int64  \n",
      " 13  BsmtHalfBath  1460 non-null   int64  \n",
      " 14  FullBath      1460 non-null   int64  \n",
      " 15  HalfBath      1460 non-null   int64  \n",
      " 16  BedroomAbvGr  1460 non-null   int64  \n",
      " 17  Fireplaces    1460 non-null   int64  \n",
      " 18  GarageArea    1460 non-null   int64  \n",
      " 19  PavedDrive    1460 non-null   object \n",
      " 20  SalePrice     1460 non-null   int64  \n",
      "dtypes: float64(1), int64(15), object(5)\n",
      "memory usage: 239.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  MSZoning Neighborhood BldgType HouseStyle PavedDrive\n",
      "0       RL      CollgCr     1Fam     2Story          Y\n",
      "1       RL      Veenker     1Fam     1Story          Y\n",
      "2       RL      CollgCr     1Fam     2Story          Y\n",
      "3       RL      Crawfor     1Fam     2Story          Y\n",
      "4       RL      NoRidge     1Fam     2Story          Y\n",
      "   MSZoning  Neighborhood  BldgType  HouseStyle  PavedDrive\n",
      "0         3             5         0           5           2\n",
      "1         3            24         0           2           2\n",
      "2         3             5         0           5           2\n",
      "3         3             6         0           5           2\n",
      "4         3            15         0           5           2\n"
     ]
    }
   ],
   "source": [
    "# Import LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Fill missing values with 0\n",
    "df.LotFrontage = df.LotFrontage.fillna(0)\n",
    "\n",
    "# Create a boolean mask for categorical columns\n",
    "categorical_mask = (df.dtypes == object)\n",
    "\n",
    "# Get list of categorical column names\n",
    "categorical_columns = df.columns[categorical_mask].tolist()\n",
    "\n",
    "# Print the head of the categorical columns\n",
    "print(df[categorical_columns].head())\n",
    "\n",
    "# Create LabelEncoder object: le\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Apply LabelEncoder to categorical columns\n",
    "df[categorical_columns] = df[categorical_columns].apply(lambda x: le.fit_transform(x))\n",
    "\n",
    "# Print the head of the LabelEncoded categorical columns\n",
    "print(df[categorical_columns].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise-19: Encoding categorical columns II: OneHotEncoder\n",
    "Okay - so you have your categorical columns encoded numerically. Can you now move onto using pipelines and XGBoost? Not yet! In the categorical columns of this dataset, there is no natural ordering between the entries. As an example: Using LabelEncoder, the CollgCr Neighborhood was encoded as 5, while the Veenker Neighborhood was encoded as 24, and Crawfor as 6. Is Veenker \"greater\" than Crawfor and CollgCr? No - and allowing the model to assume this natural ordering may result in poor performance.\n",
    "\n",
    "As a result, there is another step needed: You have to apply a one-hot encoding to create binary, or \"dummy\" variables. You can do this using scikit-learn's OneHotEncoder (https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 1.000e+00 6.000e+01 6.500e+01 8.450e+03\n",
      "  7.000e+00 5.000e+00 2.003e+03 0.000e+00 1.710e+03 1.000e+00 0.000e+00\n",
      "  2.000e+00 1.000e+00 3.000e+00 0.000e+00 5.480e+02 2.085e+05]\n",
      " [0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 1.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 1.000e+00 2.000e+01 8.000e+01 9.600e+03\n",
      "  6.000e+00 8.000e+00 1.976e+03 0.000e+00 1.262e+03 0.000e+00 1.000e+00\n",
      "  2.000e+00 0.000e+00 3.000e+00 1.000e+00 4.600e+02 1.815e+05]\n",
      " [0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 1.000e+00 6.000e+01 6.800e+01 1.125e+04\n",
      "  7.000e+00 5.000e+00 2.001e+03 1.000e+00 1.786e+03 1.000e+00 0.000e+00\n",
      "  2.000e+00 1.000e+00 3.000e+00 1.000e+00 6.080e+02 2.235e+05]\n",
      " [0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 1.000e+00 7.000e+01 6.000e+01 9.550e+03\n",
      "  7.000e+00 5.000e+00 1.915e+03 1.000e+00 1.717e+03 1.000e+00 0.000e+00\n",
      "  1.000e+00 0.000e+00 3.000e+00 1.000e+00 6.420e+02 1.400e+05]\n",
      " [0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00\n",
      "  0.000e+00 0.000e+00 0.000e+00 1.000e+00 6.000e+01 8.400e+01 1.426e+04\n",
      "  8.000e+00 5.000e+00 2.000e+03 0.000e+00 2.198e+03 1.000e+00 0.000e+00\n",
      "  2.000e+00 1.000e+00 4.000e+00 1.000e+00 8.360e+02 2.500e+05]]\n",
      "(1460, 21)\n",
      "(1460, 62)\n"
     ]
    }
   ],
   "source": [
    "# Import OneHotEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Create OneHotEncoder: ohe\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "ct = ColumnTransformer([(\"encoder\", OneHotEncoder(), categorical_columns)], remainder = 'passthrough')\n",
    "\n",
    "# Apply OneHotEncoder to categorical columns - output is no longer a dataframe: df_encoded\n",
    "df_encoded = ct.fit_transform(df)\n",
    "\n",
    "# Print first 5 rows of the resulting dataset - again, this will no longer be a pandas dataframe\n",
    "print(df_encoded[:5, :])\n",
    "\n",
    "# Print the shape of the original DataFrame\n",
    "print(df.shape)\n",
    "\n",
    "# Print the shape of the transformed array\n",
    "print(df_encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise-20: Encoding categorical columns III: DictVectorizer\n",
    "Alright, one final trick before you dive into pipelines. The two step process you just went through - LabelEncoder followed by OneHotEncoder - can be simplified by using a DictVectorizer.\n",
    "\n",
    "Using a DictVectorizer (https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html) on a DataFrame that has been converted to a dictionary allows you to get label encoding as well as one-hot encoding in one go.\n",
    "\n",
    "Your task is to work through this strategy in this exercise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 2.000e+00 5.480e+02\n",
      "  1.710e+03 1.000e+00 5.000e+00 8.450e+03 6.500e+01 6.000e+01 3.000e+00\n",
      "  5.000e+00 5.000e+00 7.000e+00 2.000e+00 0.000e+00 2.085e+05 2.003e+03]\n",
      " [3.000e+00 0.000e+00 0.000e+00 1.000e+00 1.000e+00 2.000e+00 4.600e+02\n",
      "  1.262e+03 0.000e+00 2.000e+00 9.600e+03 8.000e+01 2.000e+01 3.000e+00\n",
      "  2.400e+01 8.000e+00 6.000e+00 2.000e+00 0.000e+00 1.815e+05 1.976e+03]\n",
      " [3.000e+00 0.000e+00 1.000e+00 0.000e+00 1.000e+00 2.000e+00 6.080e+02\n",
      "  1.786e+03 1.000e+00 5.000e+00 1.125e+04 6.800e+01 6.000e+01 3.000e+00\n",
      "  5.000e+00 5.000e+00 7.000e+00 2.000e+00 1.000e+00 2.235e+05 2.001e+03]\n",
      " [3.000e+00 0.000e+00 1.000e+00 0.000e+00 1.000e+00 1.000e+00 6.420e+02\n",
      "  1.717e+03 0.000e+00 5.000e+00 9.550e+03 6.000e+01 7.000e+01 3.000e+00\n",
      "  6.000e+00 5.000e+00 7.000e+00 2.000e+00 1.000e+00 1.400e+05 1.915e+03]\n",
      " [4.000e+00 0.000e+00 1.000e+00 0.000e+00 1.000e+00 2.000e+00 8.360e+02\n",
      "  2.198e+03 1.000e+00 5.000e+00 1.426e+04 8.400e+01 6.000e+01 3.000e+00\n",
      "  1.500e+01 5.000e+00 8.000e+00 2.000e+00 0.000e+00 2.500e+05 2.000e+03]]\n",
      "{'MSSubClass': 12, 'MSZoning': 13, 'LotFrontage': 11, 'LotArea': 10, 'Neighborhood': 14, 'BldgType': 1, 'HouseStyle': 9, 'OverallQual': 16, 'OverallCond': 15, 'YearBuilt': 20, 'Remodeled': 18, 'GrLivArea': 7, 'BsmtFullBath': 2, 'BsmtHalfBath': 3, 'FullBath': 5, 'HalfBath': 8, 'BedroomAbvGr': 0, 'Fireplaces': 4, 'GarageArea': 6, 'PavedDrive': 17, 'SalePrice': 19}\n"
     ]
    }
   ],
   "source": [
    "# Import DictVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# Convert df into a dictionary: df_dict\n",
    "df_dict = df.to_dict('records')\n",
    "\n",
    "# Create the DictVectorizer object: dv\n",
    "dv = DictVectorizer(sparse=False)\n",
    "\n",
    "# Apply dv on df: df_encoded\n",
    "df_encoded = dv.fit_transform(df_dict)\n",
    "\n",
    "# Print the resulting first five rows\n",
    "print(df_encoded[:5,:])\n",
    "\n",
    "# Print the vocabulary\n",
    "print(dv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise-21: Preprocessing within a pipeline\n",
    "Now that you've seen what steps need to be taken individually to properly process the Ames housing data, let's use the much cleaner and more succinct DictVectorizer approach and put it alongside an XGBoostRegressor inside of a scikit-learn pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('ohe_onestep',\n",
       "                 DictVectorizer(dtype=<class 'numpy.float64'>, separator='=',\n",
       "                                sort=True, sparse=False)),\n",
       "                ('xgb_model',\n",
       "                 XGBRegressor(base_score=0.5, booster='gbtree',\n",
       "                              colsample_bylevel=1, colsample_bynode=1,\n",
       "                              colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "                              importance_type='gain',\n",
       "                              interaction_constraints='',\n",
       "                              learning_rate=0.300000012, max_delta_step=0,\n",
       "                              max_depth=6, min_child_weight=1, missing=nan,\n",
       "                              monotone_constraints='()', n_estimators=100,\n",
       "                              n_jobs=0, num_parallel_tree=1,\n",
       "                              objective='reg:squarederror', random_state=0,\n",
       "                              reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "                              subsample=1, tree_method='exact',\n",
       "                              validate_parameters=1, verbosity=None))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Fill LotFrontage missing values with 0\n",
    "X.LotFrontage = X.LotFrontage.fillna(0)\n",
    "\n",
    "# Setup the pipeline steps: steps\n",
    "steps = [(\"ohe_onestep\", DictVectorizer(sparse=False)),\n",
    "         (\"xgb_model\", xgb.XGBRegressor())]\n",
    "\n",
    "# Create the pipeline: xgb_pipeline\n",
    "xgb_pipeline = Pipeline(steps)\n",
    "\n",
    "# Fit the pipeline\n",
    "xgb_pipeline.fit(X.to_dict('records'), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise-22: Cross-validating your XGBoost model\n",
    "In this exercise, you'll go one step further by using the pipeline you've created to preprocess and cross-validate your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold RMSE:  27840.01103862237\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Fill LotFrontage missing values with 0\n",
    "X.LotFrontage = X.LotFrontage.fillna(0)\n",
    "\n",
    "# Setup the pipeline steps: steps\n",
    "steps = [(\"ohe_onestep\", DictVectorizer(sparse=False)),\n",
    "         (\"xgb_model\", xgb.XGBRegressor(max_depth=2, objective=\"reg:squarederror\"))]\n",
    "\n",
    "# Create the pipeline: xgb_pipeline\n",
    "xgb_pipeline = Pipeline(steps)\n",
    "\n",
    "# Cross-validate the model\n",
    "cross_val_scores = cross_val_score(xgb_pipeline, X.to_dict('records'), y, cv=10, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Print the 10-fold RMSE\n",
    "print(\"10-fold RMSE: \", np.mean(np.sqrt(np.abs(cross_val_scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise-23: Kidney disease case study I: Categorical Imputer\n",
    "You'll now continue your exploration of using pipelines with a dataset that requires significantly more wrangling. The chronic kidney disease dataset (https://archive.ics.uci.edu/ml/datasets/chronic_kidney_disease) contains both categorical and numeric features, but contains lots of missing values. The goal here is to predict who has chronic kidney disease given various blood indicators as features.\n",
    "\n",
    "As Sergey mentioned in the video, you'll be introduced to a new library, sklearn_pandas (https://github.com/scikit-learn-contrib/sklearn-pandas), that allows you to chain many more processing steps inside of a pipeline than are currently supported in scikit-learn. Specifically, you'll be able to impute missing categorical values directly using the Categorical_Imputer() class in sklearn_pandas, and the DataFrameMapper() class to apply any arbitrary sklearn-compatible transformer on DataFrame columns, where the resulting output can be either a NumPy array or DataFrame.\n",
    "\n",
    "We've also created a transformer called a Dictifier that encapsulates converting a DataFrame using .to_dict(\"records\") without you having to do it explicitly (and so that it works in a pipeline). Finally, we've also provided the list of feature names in kidney_feature_names, the target name in kidney_target_name, the features in X, and the target in y.\n",
    "\n",
    "In this exercise, your task is to apply the CategoricalImputer to impute all of the categorical columns in the dataset. You can refer to how the numeric imputation mapper was created as a template. Notice the keyword arguments input_df=True and df_out=True? This is so that you can work with DataFrames instead of arrays. By default, the transformers are passed a numpy array of the selected columns as input, and as a result, the output of the DataFrame mapper is also an array. Scikit-learn transformers have historically been designed to work with numpy arrays, not pandas DataFrames, even though their basic indexing interfaces are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>bp</th>\n",
       "      <th>sg</th>\n",
       "      <th>al</th>\n",
       "      <th>su</th>\n",
       "      <th>bgr</th>\n",
       "      <th>bu</th>\n",
       "      <th>sc</th>\n",
       "      <th>sod</th>\n",
       "      <th>pot</th>\n",
       "      <th>...</th>\n",
       "      <th>rbc</th>\n",
       "      <th>pc</th>\n",
       "      <th>pcc</th>\n",
       "      <th>ba</th>\n",
       "      <th>htn</th>\n",
       "      <th>dm</th>\n",
       "      <th>cad</th>\n",
       "      <th>appet</th>\n",
       "      <th>pe</th>\n",
       "      <th>ane</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>80</td>\n",
       "      <td>1.020</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>?</td>\n",
       "      <td>normal</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>121</td>\n",
       "      <td>36</td>\n",
       "      <td>...</td>\n",
       "      <td>44</td>\n",
       "      <td>7800</td>\n",
       "      <td>5.2</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>good</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>ckd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>50</td>\n",
       "      <td>1.020</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>?</td>\n",
       "      <td>normal</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>?</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>6000</td>\n",
       "      <td>?</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>good</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>ckd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>80</td>\n",
       "      <td>1.010</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>423</td>\n",
       "      <td>53</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>7500</td>\n",
       "      <td>?</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>poor</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>ckd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>70</td>\n",
       "      <td>1.005</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>abnormal</td>\n",
       "      <td>present</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>117</td>\n",
       "      <td>56</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>6700</td>\n",
       "      <td>3.9</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>poor</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>ckd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>80</td>\n",
       "      <td>1.010</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>notpresent</td>\n",
       "      <td>106</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>35</td>\n",
       "      <td>7300</td>\n",
       "      <td>4.6</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>good</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>ckd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age     bp sg al      su       bgr          bu          sc  sod pot  ...  \\\n",
       "48  80  1.020  1  0       ?    normal  notpresent  notpresent  121  36  ...   \n",
       "7   50  1.020  4  0       ?    normal  notpresent  notpresent    ?  18  ...   \n",
       "62  80  1.010  2  3  normal    normal  notpresent  notpresent  423  53  ...   \n",
       "48  70  1.005  4  0  normal  abnormal     present  notpresent  117  56  ...   \n",
       "51  80  1.010  2  0  normal    normal  notpresent  notpresent  106  26  ...   \n",
       "\n",
       "   rbc    pc  pcc   ba  htn  dm   cad appet   pe  ane  \n",
       "48  44  7800  5.2  yes  yes  no  good    no   no  ckd  \n",
       "7   38  6000    ?   no   no  no  good    no   no  ckd  \n",
       "62  31  7500    ?   no  yes  no  poor    no  yes  ckd  \n",
       "48  32  6700  3.9  yes   no  no  poor   yes  yes  ckd  \n",
       "51  35  7300  4.6   no   no  no  good    no   no  ckd  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('chronic_kidney_disease.csv', names=['age', 'bp', 'sg', 'al', 'su', 'bgr', 'bu', 'sc', 'sod', 'pot', \n",
    "                                                      'hemo', 'pcv', 'wc', 'rc', 'rbc', 'pc', 'pcc', 'ba', 'htn', 'dm', \n",
    "                                                      'cad', 'appet', 'pe', 'ane'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.drop('ane', axis=1), df['ane']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age      0\n",
      "bp       0\n",
      "sg       0\n",
      "al       0\n",
      "su       0\n",
      "bgr      0\n",
      "bu       0\n",
      "sc       0\n",
      "sod      0\n",
      "pot      0\n",
      "hemo     0\n",
      "pcv      0\n",
      "wc       0\n",
      "rc       0\n",
      "rbc      0\n",
      "pc       0\n",
      "pcc      0\n",
      "ba       0\n",
      "htn      0\n",
      "dm       0\n",
      "cad      0\n",
      "appet    0\n",
      "pe       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn_pandas import CategoricalImputer\n",
    "from sklearn.impute import SimpleImputer as Imputer\n",
    "\n",
    "# Check number of nulls in each feature column\n",
    "nulls_per_column = X.isnull().sum()\n",
    "print(nulls_per_column)\n",
    "\n",
    "# Create a boolean mask for categorical columns\n",
    "categorical_feature_mask = X.dtypes == object\n",
    "\n",
    "# Get list of categorical column names\n",
    "categorical_columns = X.columns[categorical_feature_mask].tolist()\n",
    "\n",
    "# Get list of non-categorical column names\n",
    "non_categorical_columns = X.columns[~categorical_feature_mask].tolist()\n",
    "\n",
    "# Apply numeric imputer\n",
    "numeric_imputation_mapper = DataFrameMapper([([numeric_feature], Imputer(strategy=\"median\")) \n",
    "                                             for numeric_feature in non_categorical_columns], input_df=True, df_out=True)\n",
    "\n",
    "# Apply categorical imputer\n",
    "categorical_imputation_mapper = DataFrameMapper([(category_feature, CategoricalImputer()) \n",
    "                                                 for category_feature in categorical_columns], input_df=True, df_out=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise-24: Kidney disease case study II: Feature Union\n",
    "Having separately imputed numeric as well as categorical columns, your task is now to use scikit-learn's FeatureUnion (https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html) to concatenate their results, which are contained in two separate transformer objects - numeric_imputation_mapper, and categorical_imputation_mapper, respectively.\n",
    "\n",
    "You may have already encountered FeatureUnion in Machine Learning with the Experts: School Budgets (https://campus.datacamp.com/courses/case-study-school-budgeting-with-machine-learning-in-python/improving-your-model?ex=7). Just like with pipelines, you have to pass it a list of (string, transformer) tuples, where the first half of each tuple is the name of the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import FeatureUnion\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# Combine the numeric and categorical transformations\n",
    "numeric_categorical_union = FeatureUnion([(\"num_mapper\", numeric_imputation_mapper),\n",
    "                                          (\"cat_mapper\", categorical_imputation_mapper)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise-25: Kidney disease case study III: Full pipeline\n",
    "It's time to piece together all of the transforms along with an XGBClassifier to build the full pipeline!\n",
    "\n",
    "Besides the numeric_categorical_union that you created in the previous exercise, there are two other transforms needed: the Dictifier() transform which we created for you, and the DictVectorizer().\n",
    "\n",
    "After creating the pipeline, your task is to cross-validate it to see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn_pandas import cross_val_score\n",
    "\n",
    "class Dictifier(BaseEstimator, TransformerMixin):       \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if type(X) == pd.core.frame.DataFrame:\n",
    "            return X.to_dict(\"records\")\n",
    "        else:\n",
    "            return pd.DataFrame(X).to_dict(\"records\")\n",
    "\n",
    "# Create full pipeline\n",
    "pipeline = Pipeline([(\"featureunion\", numeric_categorical_union),\n",
    "                     (\"dictifier\", Dictifier()),\n",
    "                     (\"vectorizer\", DictVectorizer(sort=False)),\n",
    "                     (\"clf\", xgb.XGBClassifier(max_depth=3))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Perform cross-validation\n",
    "cross_val_scores = cross_val_score(pipeline, X, y, scoring=\"roc_auc\", cv=3)\n",
    "\n",
    "# Print avg. AUC\n",
    "print(\"3-fold AUC: \", np.mean(cross_val_scores))\n",
    "```\n",
    "\n",
    "<code>3-fold AUC:  0.998637406769937</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise-26: Bringing it all together\n",
    "Alright, it's time to bring together everything you've learned so far! In this final exercise of the course, you will combine your work from the previous exercises into one end-to-end XGBoost pipeline to really cement your understanding of preprocessing and pipelines in XGBoost.\n",
    "\n",
    "Your work from the previous 3 exercises, where you preprocessed the data and set up your pipeline, has been pre-loaded. Your job is to perform a randomized search and identify the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid\n",
    "gbm_param_grid = {\n",
    "    'clf__learning_rate': np.arange(0.05, 1, 0.05),\n",
    "    'clf__max_depth': np.arange(3, 10, 1),\n",
    "    'clf__n_estimators': np.arange(50, 200, 50)\n",
    "}\n",
    "\n",
    "# Perform RandomizedSearchCV\n",
    "randomized_roc_auc = RandomizedSearchCV(pipeline, param_distributions=gbm_param_grid, n_iter=2, \n",
    "                                        scoring='roc_auc', verbose=1, cv=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Fit the estimator\n",
    "randomized_roc_auc.fit(X, y)\n",
    "\n",
    "# Compute metrics\n",
    "print(randomized_roc_auc.best_score_)\n",
    "print(randomized_roc_auc.best_estimator_)\n",
    "```\n",
    "\n",
    "<code>0.9965333333333334</code>\n",
    "\n",
    "<code> XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
    "              learning_rate=0.9500000000000001, max_delta_step=0, max_depth=4, min_child_weight=1, missing=None,\n",
    "              n_estimators=100, n_jobs=1, nthread=None, objective='binary:logistic', random_state=0, \n",
    "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None, silent=None, subsample=1, verbosity=1)</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THE END!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
